{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 9: Final Project\n",
    "## Computational Methods in Psychology (and Neuroscience)\n",
    "### Psychology 4500/7559 --- Fall 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives\n",
    "\n",
    "Upon completion of this assignment, students will have:\n",
    "\n",
    "1. Described the list generation process in detail\n",
    "2. Described the experiment details\n",
    "3. Visualized processed data\n",
    "4. Performed a statistical analysis to test the hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment\n",
    "\n",
    "Write text (in MarkDown cells) and code (in Code cells) in a Jupyter notebook (after making a copy and renaming it to have your userid in the title --- e.g., A09_Final_Project_mst3k).\n",
    "\n",
    "\n",
    "## Details\n",
    "\n",
    "The goal of the final project is to synthesize material covered in the class and produce part of what would go into an actual scientific publication based on *one* of the experiments we ran in the class. Specifically, you will be writing part of the Methods and Results sections.\n",
    "\n",
    "The basic template is below the code for loading and processing the data. There we outline what each section should include. As always, make sure to label all figures and be sure to refer to the code in the lesson notebooks as a guide for your analyses.\n",
    "\n",
    "Please feel free to reach out to us on Slack if you have any questions along the way.\n",
    "\n",
    "* ***When you are done, save this notebook as HTML (`File -> Download as -> HTML`) and upload it to the matching assignment on UVACollab.***  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some useful libraries\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "import pandas as pd               # efficient tables\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "import plotnine as pn \n",
    "import scipy.stats.distributions as dists     # probability distributions\n",
    "from scipy import stats\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from smile.log import log2dl\n",
    "\n",
    "from ci_within import ci_within\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Custom SLOG loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function to load slogs\n",
    "def load_all_subj_logs(task_dir, log_file):\n",
    "    # load in a list of all the subj\n",
    "    subjs = [os.path.split(subj_dir)[-1] \n",
    "             for subj_dir in glob(os.path.join(task_dir, 's*'))]\n",
    "    subjs.sort()\n",
    "\n",
    "    # loop over subj and their data\n",
    "    all_dat = []\n",
    "    for subj in subjs:\n",
    "        # set the file\n",
    "        log_path = os.path.join(task_dir, subj, log_file)\n",
    "        #print(log_path)\n",
    "\n",
    "        # load the data\n",
    "        all_dat.extend(log2dl(log_path, subj=subj))\n",
    "\n",
    "    df = pd.DataFrame(all_dat)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load in all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resp_map_lure</th>\n",
       "      <th>resp_map_target</th>\n",
       "      <th>block_num</th>\n",
       "      <th>trial_num</th>\n",
       "      <th>stim_on_time</th>\n",
       "      <th>stim_on_error</th>\n",
       "      <th>resp</th>\n",
       "      <th>resp_time_time</th>\n",
       "      <th>resp_time_error</th>\n",
       "      <th>rt</th>\n",
       "      <th>...</th>\n",
       "      <th>valence_sd</th>\n",
       "      <th>arousal_mean</th>\n",
       "      <th>arousal_sd</th>\n",
       "      <th>dominance_mean</th>\n",
       "      <th>dominance_sd</th>\n",
       "      <th>word_frequency</th>\n",
       "      <th>novelty</th>\n",
       "      <th>cond</th>\n",
       "      <th>subj</th>\n",
       "      <th>log_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>234.395511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>J</td>\n",
       "      <td>235.284833</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.889323</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5700000000000001</td>\n",
       "      <td>5.3099999999999996</td>\n",
       "      <td>2.23</td>\n",
       "      <td>5.46</td>\n",
       "      <td>2.0499999999999998</td>\n",
       "      <td>3</td>\n",
       "      <td>target</td>\n",
       "      <td>neu</td>\n",
       "      <td>s001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>235.885654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>237.034670</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>1.149016</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.1200000000000001</td>\n",
       "      <td>1.8300000000000001</td>\n",
       "      <td>5.6600000000000001</td>\n",
       "      <td>1.78</td>\n",
       "      <td>12</td>\n",
       "      <td>lure</td>\n",
       "      <td>neu</td>\n",
       "      <td>s001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>237.616869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>238.767406</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>1.150537</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8200000000000001</td>\n",
       "      <td>5.4500000000000002</td>\n",
       "      <td>2.1499999999999999</td>\n",
       "      <td>4.6399999999999997</td>\n",
       "      <td>2.0699999999999998</td>\n",
       "      <td>16</td>\n",
       "      <td>lure</td>\n",
       "      <td>neu</td>\n",
       "      <td>s001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>239.624933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>240.432295</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.807362</td>\n",
       "      <td>...</td>\n",
       "      <td>1.24</td>\n",
       "      <td>3.9500000000000002</td>\n",
       "      <td>2.5800000000000001</td>\n",
       "      <td>5.3700000000000001</td>\n",
       "      <td>1.6399999999999999</td>\n",
       "      <td>19</td>\n",
       "      <td>lure</td>\n",
       "      <td>neu</td>\n",
       "      <td>s001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>241.432209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>242.545227</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>1.113017</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1600000000000001</td>\n",
       "      <td>3.6800000000000002</td>\n",
       "      <td>2.5699999999999998</td>\n",
       "      <td>5.8300000000000001</td>\n",
       "      <td>1.5</td>\n",
       "      <td>49</td>\n",
       "      <td>lure</td>\n",
       "      <td>neu</td>\n",
       "      <td>s001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  resp_map_lure resp_map_target  block_num  trial_num  stim_on_time  \\\n",
       "0             F               J          0          0    234.395511   \n",
       "1             F               J          0          1    235.885654   \n",
       "2             F               J          0          2    237.616869   \n",
       "3             F               J          0          3    239.624933   \n",
       "4             F               J          0          4    241.432209   \n",
       "\n",
       "   stim_on_error resp  resp_time_time  resp_time_error        rt  ...  \\\n",
       "0            0.0    J      235.284833         0.000180  0.889323  ...   \n",
       "1            0.0    F      237.034670         0.000182  1.149016  ...   \n",
       "2            0.0    F      238.767406         0.000238  1.150537  ...   \n",
       "3            0.0    F      240.432295         0.000182  0.807362  ...   \n",
       "4            0.0    F      242.545227         0.000192  1.113017  ...   \n",
       "\n",
       "           valence_sd        arousal_mean          arousal_sd  \\\n",
       "0  1.5700000000000001  5.3099999999999996                2.23   \n",
       "1                 1.5  4.1200000000000001  1.8300000000000001   \n",
       "2  1.8200000000000001  5.4500000000000002  2.1499999999999999   \n",
       "3                1.24  3.9500000000000002  2.5800000000000001   \n",
       "4  2.1600000000000001  3.6800000000000002  2.5699999999999998   \n",
       "\n",
       "       dominance_mean        dominance_sd word_frequency novelty cond  subj  \\\n",
       "0                5.46  2.0499999999999998              3  target  neu  s001   \n",
       "1  5.6600000000000001                1.78             12    lure  neu  s001   \n",
       "2  4.6399999999999997  2.0699999999999998             16    lure  neu  s001   \n",
       "3  5.3700000000000001  1.6399999999999999             19    lure  neu  s001   \n",
       "4  5.8300000000000001                 1.5             49    lure  neu  s001   \n",
       "\n",
       "  log_num  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data from the word recog task\n",
    "task_dir = os.path.join('..', 'lessons', 'data2', 'Taskapalooza')\n",
    "\n",
    "df_f = load_all_subj_logs(task_dir, 'log_flanker')\n",
    "df_i = load_all_subj_logs(task_dir, 'log_image_test')\n",
    "df_w = load_all_subj_logs(task_dir, 'log_word_test')\n",
    "df_w.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some data clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it turns out the cond is easier to visualize as pure and mixed\n",
    "def fix_conds(df, type_col):\n",
    "    # loop over the unique subjects\n",
    "    usubj = df.subj.unique()\n",
    "    for s in usubj:\n",
    "        # loop over their blocks\n",
    "        ublocks = df.loc[df['subj']==s, 'block_num'].unique()\n",
    "        for b in ublocks:\n",
    "            # grab the data for that subj and block\n",
    "            dfb = df.loc[(df['subj']==s)&(df['block_num']==b)]\n",
    "            \n",
    "            # get the unique types in that block\n",
    "            uval = dfb[type_col].unique()\n",
    "            if len(uval) > 1:\n",
    "                # it's mixed\n",
    "                df.loc[(df['subj']==s)&(df.block_num==b), 'cond'] = 'mixed'\n",
    "            else:\n",
    "                # it's the pure\n",
    "                df.loc[(df['subj']==s)&(df.block_num==b), 'cond'] = 'pure'\n",
    "\n",
    "# fix the conds in the recog experiments (updated in place)\n",
    "fix_conds(df_i, type_col='in_out')\n",
    "fix_conds(df_w, type_col='valence')\n",
    "\n",
    "# add in log_rt columns\n",
    "df_f['log_rt'] = np.log(df_f['rt'])\n",
    "df_i['log_rt'] = np.log(df_i['rt'])\n",
    "df_w['log_rt'] = np.log(df_w['rt'])\n",
    "\n",
    "# must make correct an int\n",
    "df_f['correct'] = df_f['correct'].astype(np.int)\n",
    "df_i['correct'] = df_i['correct'].astype(np.int)\n",
    "df_w['correct'] = df_w['correct'].astype(np.int)\n",
    "\n",
    "# add in a column for whether they made an 'old' response\n",
    "df_i['old_resp'] = (df_i['resp_map_target'] == df_i['resp']).astype(np.int)\n",
    "df_w['old_resp'] = (df_w['resp_map_target'] == df_w['resp']).astype(np.int)\n",
    "\n",
    "# process some of the valence info\n",
    "df_w['valence_mean'] = df_w['valence_mean'].astype(np.float)\n",
    "df_w['arousal_mean'] = df_w['arousal_mean'].astype(np.float)\n",
    "df_w['dominance_mean'] = df_w['dominance_mean'].astype(np.float)\n",
    "df_w['abs_valence'] = np.abs(df_w['valence_mean'] - 5.0)\n",
    "df_w['abs_arousal'] = np.abs(df_w['arousal_mean'] - 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis\n",
    "\n",
    "The hypothesis of this experiment is the following:\n",
    "\n",
    " **There is an effect of valence and correctness (potentially interacting with condition) on response times.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Methods\n",
    "\n",
    "\n",
    "\n",
    "## List generation\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal of the List Generation procedure is to create usable test and study lists that can be presented to participants in the Memory Recognition task below. The inputs of this procedure are pools of stimuli split up by valence and the output is a list of blocks containing study and test list pairs of stimuli. \n",
    "\n",
    "\n",
    "### Requirements and Data\n",
    "\n",
    "This List Generation procedure uses imported python libraries such as `csv` to read in the pools of stimuli that are split up by valence. \n",
    "\n",
    "The primary pools of data used were contained in the following three CSV files:\n",
    "- [Positive Pool](./pos_pool.csv)\n",
    "- [Negative Pool](./neg_pool.csv)\n",
    "- [Neutral Pool](./neu_pool.csv)\n",
    "\n",
    "The underlying Memory Recognition task is set up as a study-test design with two primary conditions: `pure` and `mixed`. The pure condition contains study and test lists of the same valence: either `positive`, `negative` or `neutral` valences. The mixed condition contains study and test lists with varying valences. \n",
    "\n",
    "Important requirements for the lists are as follows:\n",
    "1. All study lists for both `pure` and `mixed` block conditions should have an equal number of words\n",
    "2. All test lists for both `pure` and `mixed` block conditions should have an equal number of words\n",
    "3. There should be an equal number of `positive`, `negative` or `neutral` valenced words in both the study and test lists of the mixed condition blocks. \n",
    "4. There should be no repeated words across the blocks of the experiment\n",
    "\n",
    "\n",
    "### Design \n",
    "\n",
    "To produce the appropriate data for this experiment, the List Generation procedure builds the experimental blocks from the bottom-up, starting with individual study and test words. Individual words were stored such that all the relevant information of the particular stimulus including its novelty, condition and valence were saved as well. For this particular procedure we utilized a python dictionary to accomplish this, but any similar data structure would do.  \n",
    "\n",
    "\n",
    "These dictionaries are then grouped into into study and test lists that are further grouped into study and test list pairs representing blocks. **Note: each block was a pair containing a study list and a test list**. Because the Memory Recognition task requires an equal number of positive pure, negative pure, neutral pure and mixed blocks, blocks are added to the `blocks` list in increments of 4 (1 for each block type). `blocks` was the finalized list and output for this procedure\n",
    "\n",
    "\n",
    "\n",
    "### Procedure \n",
    "\n",
    "Once the data pools are read in, the appropriate configuration variables are set up including the number of pools(or valences), number of items per pure list, and number of repetitions for each block type. \n",
    "\n",
    "Another important variable is the number of items per valence in the mixed list. This should be set equal to number of items per pure list divided by the number of pools. This is done to ensure that every type of block has eqiuivalent list sizes. It is thus crucial to make sure that our number of items per pure list is configured so that it is divisible by the number of pools. For this experiment, 3 pools were used (positive, negative and neutral), the number of items per pure list was 24 and the number of repeitions for each block type was 2. Because we have 4 types of blocks (positive pure, negative pure, neutral pure and mixed blocks) our experiment had 8 blocks total. \n",
    "\n",
    "Once the configuration variables were set up appropriately, the data pools for each valence were taken in as lists and a valence attribute was added to each element in the respective valence pools to signify whether they were positive, negative or neutral. The blocks were subsequently shuffled to maintain randomization. \n",
    "\n",
    "Then, using a dedicated block generation function and the number of repetitions per block type (2 in this case), the 8 blocks of the experiment were created. The block generation function took in a list of pools, a valence, and a number of items. For the pure lists, only one pool assosiated with the valence was passed in and the number of items passed in was the pre-configured number of items per pure list. For the mixed lists, all three pools were passed in and the number of items passed in was the pre-configured number of items per mixed list. \n",
    "\n",
    "Using the passed in parameters, the block generation function looped through the provided pools (note: mixed lists had 3x more iterations to accomodate for the smaller number of items passed in) and created **indiviual dictionaries for every study item**, using the data from the given pools and storing relevant information such as the item's condition, novelty and valence. These study word dictionaries were then added to a study list containing all the study words for the particular block. Once every study word was added, the list was shuffled to maintain randomization. \n",
    "\n",
    "The test list for a particular block was created by creating a copy of the study list, and looping through the pools of data again to add an equivalent number of new words (lures). The test list was shuffled at the end to maintain randomization. \n",
    "\n",
    "Finally, the block generation function returned a dictionary containing both the study and test lists. This is was repeated over the number of required blocks (8 blocks total with 2 of each block type), and the final output was shuffled to maintain inter-block randomization. \n",
    "\n",
    "\n",
    "\n",
    "## SMILE Experiment Details\n",
    "\n",
    "### Objective\n",
    "\n",
    "This Memory Recognition task utilizes the lists generated by code from the list generation procedure above to create an experiment for collecting the desired data. For this Recognition Memory task, participants will study a list of items one at a time, and then, after a short math distractor, wil be tested for their memory of those items. \n",
    "\n",
    "\n",
    "### Requirements and Data\n",
    "\n",
    "The Recognition Memory task was created using an open-source, Python-based experient bulding library: State Machine Interface Library for Experiments (SMILE; https://github.com/compmem/smile). Subjects completed the task on their local computers that were capable of running the provided experiments. \n",
    "\n",
    "\n",
    "All data imported for the study and test lists are taken from the \"List Generation\" procedure above. The required study and test lists are taken in as a variable `blocks` which represents the blocks of trials of this experiment. \n",
    "\n",
    "\n",
    "    \n",
    "### Design \n",
    "\n",
    "The Recognition Memory task utilizes a study-test design with two primary conditions: pure and mixed. The pure condition contains study and test words of the same valence: either positive, negative or neutral valences. The mixed condition contains study and test words of varying valences. \n",
    "\n",
    "On each trial, subjects were presented with a list of study words and were tested on a matching list of test words that contained the study words (targets) and a set of new unstudied words (lures). In between the study and test phases, subjects were given a short (set time) math distractor in which they signified whether a presented math equation was valid or invalid by pressing the \"F\" key for valid equations and \"J\" key for invalid equations. In the testing phase, subjects were asked to make a decision on whether thay had studied the presented words by pressing either the \"F\" key for targets or the \"J\" key for lures. \n",
    "\n",
    "Subjects completed 8 blocks, consiting of 24 study words and 48 test words each. There were 4 primary block types and there were an equal number (2 each) of each block type: positive pure, negative pure, neutral pure and mixed. \n",
    "\n",
    "\n",
    "### Procedure\n",
    "\n",
    "After inputing their subject id information, subjects were presented with a brief instruction screen for the experiment. The instructions contained information about the Memory Recognition task, introducing the study phase, testing phase and the 30 seconds of math problems in between them. After the subjects clicked through the instruction screen, they were taken to the start of the first block.\n",
    "\n",
    "At the start of a block, the subjects are taken to a pre-study-phase screen prompting the users to hit the \"ENTER\" key prior to beginning the study phase. Every study phase started with a pre-configured delay time including a jitter. Within the study phase itself, each item is presented for 1s and after each study item was presented, there was an inter stimulus delay (default was 0.5s) along with a randomized jitter (random value up to 0.5s)\n",
    "\n",
    "Following the study phase, the subjects were taken to an instruction screen for the math portion of the block, which included a prompt for the users to hit the \"ENTER\" key prior to beginning the math portion. \n",
    "\n",
    "Finally, after the math portion of the block, subjects were taken to an instruction screen for the testing phase of the block, including a prompt for the users to hit the \"ENTER\" key prior to beginning the testing phase. \n",
    "\n",
    "\n",
    "During the testing phase, subjects were presented with a stimulus that was either a studied item (target) or a new item (lure). Using the designated response keys, \"F\" for targets, \"J\" for lures, subjects made decisions on whether they had studied the presented words. After each test item was presented and responded to, there was an inter stimulus delay (default was 0.5s) along with a randomized jitter (random value up to 0.5s)\n",
    "\n",
    "\n",
    "Upon completion of the testing phase, the next block starts in the same way, and the cycle is repeated until all the blocks are finished. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "## Primary Question: \n",
    "Is there an effect of valence (potentially interacting with condition) and correctness on response times?\n",
    "\n",
    "## Motivations \n",
    "Before running this analysis, I first want to introduce the motivations for this hypothesis. Thinking about the nature of the recognition memory test, there is some reason to believe that people process different words at different rates. Particularly, positive, negative and neutral words may be percieved diffently based on one's personal experiences and could subsequently be processed differently. Additionally, there is reason to believe that incorrect responses had longer response times bcause the individual was potentially unsure of their response. \n",
    "Though causality can never be guaranteed in either of these cases, this analysis looks to see if there is some tangible effects in the data. From there, further analyses can be run to find more conclsive findings. \n",
    "\n",
    "## Important Variables\n",
    "In this analysis, the dependent variable will be `log_rt` instead of `rt` because logarithmic values are easier to analyze and interpret when using statistical analyses. For the independent variables, we will use `valence` and `correct`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data processing and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ggplot: (7012048969)>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the right dataframe to plot and visualize the problem stated above\n",
    "# get the error corrected by condition and whether they answered correctly\n",
    "res = ci_within(df_w,  \n",
    "                indexvar='subj',       # column that identifies a subject\n",
    "                withinvars=['cond', 'correct', 'valence'],     # list of columns for grouping within subject\n",
    "                measvar='log_rt')        # dependent variable averaging over\n",
    "\n",
    "res = res.reset_index()\n",
    "\n",
    "p = (pn.ggplot(res, pn.aes('cond', 'mean', fill='valence'))\n",
    "     + pn.geom_errorbar(pn.aes(ymin='mean-ci', ymax='mean+ci', width=0.2), \n",
    "                        position=pn.position_dodge(.7))\n",
    "     + pn.geom_point(position=pn.position_dodge(.7), size=4)\n",
    "     + pn.facet_wrap('~ correct')\n",
    "     + pn.labs(title=\"Figure 1: Plot of log_rt by valence across correctness\", x=\"Condition\", y = \"P(log_rt)\", fill='Valence')\n",
    "    )\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the figure above, we can see a plot of `log_rt` as a function of valence. The plot is split into correct and incorrect responses (labeled by the `1` and `0` at the top of each plot respectively). Furthermore, to account for differences in condition, two sets of plots (`mixed` and `pure`) were included for each valence and corectness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical test and interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>cond</th>\n",
       "      <th>valence</th>\n",
       "      <th>correct</th>\n",
       "      <th>log_rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s001</td>\n",
       "      <td>mixed</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s001</td>\n",
       "      <td>mixed</td>\n",
       "      <td>neg</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.217212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s001</td>\n",
       "      <td>mixed</td>\n",
       "      <td>neu</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.109757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s001</td>\n",
       "      <td>mixed</td>\n",
       "      <td>neu</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.215241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s001</td>\n",
       "      <td>mixed</td>\n",
       "      <td>pos</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>s023</td>\n",
       "      <td>pure</td>\n",
       "      <td>neg</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.284881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>s023</td>\n",
       "      <td>pure</td>\n",
       "      <td>neu</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.249797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>s023</td>\n",
       "      <td>pure</td>\n",
       "      <td>neu</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.347838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>s023</td>\n",
       "      <td>pure</td>\n",
       "      <td>pos</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.357336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>s023</td>\n",
       "      <td>pure</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.447838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subj   cond valence  correct    log_rt\n",
       "0    s001  mixed     neg        0  0.167138\n",
       "1    s001  mixed     neg        1 -0.217212\n",
       "2    s001  mixed     neu        0 -0.109757\n",
       "3    s001  mixed     neu        1 -0.215241\n",
       "4    s001  mixed     pos        0  0.166381\n",
       "..    ...    ...     ...      ...       ...\n",
       "271  s023   pure     neg        1 -0.284881\n",
       "272  s023   pure     neu        0 -0.249797\n",
       "273  s023   pure     neu        1 -0.347838\n",
       "274  s023   pure     pos        0 -0.357336\n",
       "275  s023   pure     pos        1 -0.447838\n",
       "\n",
       "[276 rows x 5 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for statistical test using statsmodel\n",
    "\n",
    "# use the agg method to get the means\n",
    "perf = df_w.groupby(['subj', 'cond', 'valence', 'correct'])['log_rt'].mean()\n",
    "perf = perf.reset_index()\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>log_rt</td>      <th>  R-squared:         </th> <td>   0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 02 Dec 2020</td> <th>  Prob (F-statistic):</th> <td>1.70e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:59:04</td>     <th>  Log-Likelihood:    </th> <td> -52.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   276</td>      <th>  AIC:               </th> <td>   128.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   264</td>      <th>  BIC:               </th> <td>   172.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                   <td></td>                      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                           <td>    0.0358</td> <td>    0.062</td> <td>    0.574</td> <td> 0.566</td> <td>   -0.087</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cond[T.pure]</th>                        <td>   -0.0377</td> <td>    0.088</td> <td>   -0.427</td> <td> 0.670</td> <td>   -0.211</td> <td>    0.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence[T.neu]</th>                      <td>   -0.0979</td> <td>    0.088</td> <td>   -1.110</td> <td> 0.268</td> <td>   -0.272</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence[T.pos]</th>                      <td>    0.0480</td> <td>    0.088</td> <td>    0.544</td> <td> 0.587</td> <td>   -0.126</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cond[T.pure]:valence[T.neu]</th>         <td>    0.1971</td> <td>    0.125</td> <td>    1.580</td> <td> 0.115</td> <td>   -0.049</td> <td>    0.443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cond[T.pure]:valence[T.pos]</th>         <td>   -0.0671</td> <td>    0.125</td> <td>   -0.538</td> <td> 0.591</td> <td>   -0.313</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>correct</th>                             <td>   -0.2348</td> <td>    0.088</td> <td>   -2.661</td> <td> 0.008</td> <td>   -0.409</td> <td>   -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cond[T.pure]:correct</th>                <td>    0.0233</td> <td>    0.125</td> <td>    0.186</td> <td> 0.852</td> <td>   -0.222</td> <td>    0.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence[T.neu]:correct</th>              <td>    0.0741</td> <td>    0.125</td> <td>    0.594</td> <td> 0.553</td> <td>   -0.172</td> <td>    0.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence[T.pos]:correct</th>              <td>   -0.0286</td> <td>    0.125</td> <td>   -0.229</td> <td> 0.819</td> <td>   -0.274</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cond[T.pure]:valence[T.neu]:correct</th> <td>   -0.1750</td> <td>    0.176</td> <td>   -0.992</td> <td> 0.322</td> <td>   -0.522</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cond[T.pure]:valence[T.pos]:correct</th> <td>    0.0680</td> <td>    0.176</td> <td>    0.385</td> <td> 0.700</td> <td>   -0.279</td> <td>    0.415</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>120.457</td> <th>  Durbin-Watson:     </th> <td>   0.866</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 535.822</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.794</td>  <th>  Prob(JB):          </th> <td>4.44e-117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.807</td>  <th>  Cond. No.          </th> <td>    25.6</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 log_rt   R-squared:                       0.144\n",
       "Model:                            OLS   Adj. R-squared:                  0.109\n",
       "Method:                 Least Squares   F-statistic:                     4.053\n",
       "Date:                Wed, 02 Dec 2020   Prob (F-statistic):           1.70e-05\n",
       "Time:                        22:59:04   Log-Likelihood:                -52.458\n",
       "No. Observations:                 276   AIC:                             128.9\n",
       "Df Residuals:                     264   BIC:                             172.4\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=======================================================================================================\n",
       "                                          coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------------\n",
       "Intercept                               0.0358      0.062      0.574      0.566      -0.087       0.159\n",
       "cond[T.pure]                           -0.0377      0.088     -0.427      0.670      -0.211       0.136\n",
       "valence[T.neu]                         -0.0979      0.088     -1.110      0.268      -0.272       0.076\n",
       "valence[T.pos]                          0.0480      0.088      0.544      0.587      -0.126       0.222\n",
       "cond[T.pure]:valence[T.neu]             0.1971      0.125      1.580      0.115      -0.049       0.443\n",
       "cond[T.pure]:valence[T.pos]            -0.0671      0.125     -0.538      0.591      -0.313       0.179\n",
       "correct                                -0.2348      0.088     -2.661      0.008      -0.409      -0.061\n",
       "cond[T.pure]:correct                    0.0233      0.125      0.186      0.852      -0.222       0.269\n",
       "valence[T.neu]:correct                  0.0741      0.125      0.594      0.553      -0.172       0.320\n",
       "valence[T.pos]:correct                 -0.0286      0.125     -0.229      0.819      -0.274       0.217\n",
       "cond[T.pure]:valence[T.neu]:correct    -0.1750      0.176     -0.992      0.322      -0.522       0.172\n",
       "cond[T.pure]:valence[T.pos]:correct     0.0680      0.176      0.385      0.700      -0.279       0.415\n",
       "==============================================================================\n",
       "Omnibus:                      120.457   Durbin-Watson:                   0.866\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              535.822\n",
       "Skew:                           1.794   Prob(JB):                    4.44e-117\n",
       "Kurtosis:                       8.807   Cond. No.                         25.6\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a linear regression of the full model\n",
    "m0 = smf.ols(\"log_rt ~ cond * valence * correct\", perf).fit()\n",
    "m0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running an ANOVA on the linear model results\n",
    "\n",
    "- The results of such a complicated linear model is hard to unpack\n",
    "- The most common approach to modeling the data would be a repeated measures ANOVA\n",
    "- Luckily, a linear regress is effectively just an ANOVA if you make the right comparisons\n",
    "- To simplify the results of the regression, we will run an ANOVA analysis below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>df</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cond</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>9.886632e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valence</th>\n",
       "      <td>0.026583</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.148474</td>\n",
       "      <td>8.620946e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cond:valence</th>\n",
       "      <td>0.256683</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.433668</td>\n",
       "      <td>2.402830e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct</th>\n",
       "      <td>3.519608</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.316559</td>\n",
       "      <td>1.463755e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cond:correct</th>\n",
       "      <td>0.002658</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.029688</td>\n",
       "      <td>8.633310e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valence:correct</th>\n",
       "      <td>0.004318</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.024119</td>\n",
       "      <td>9.761714e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cond:valence:correct</th>\n",
       "      <td>0.180761</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.009613</td>\n",
       "      <td>3.657622e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>23.633208</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sum_sq     df          F        PR(>F)\n",
       "cond                   0.000018    1.0   0.000202  9.886632e-01\n",
       "valence                0.026583    2.0   0.148474  8.620946e-01\n",
       "cond:valence           0.256683    2.0   1.433668  2.402830e-01\n",
       "correct                3.519608    1.0  39.316559  1.463755e-09\n",
       "cond:correct           0.002658    1.0   0.029688  8.633310e-01\n",
       "valence:correct        0.004318    2.0   0.024119  9.761714e-01\n",
       "cond:valence:correct   0.180761    2.0   1.009613  3.657622e-01\n",
       "Residual              23.633208  264.0        NaN           NaN"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run a type-II repeated measures ANOVA based on the linear model results\n",
    "sm.stats.anova_lm(m0, typ=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings\n",
    "\n",
    "Starting with the plots presented in Figure 1 above, it is clear that there is a discrepancy in response times for responses across corectness. Specfically, we see that incorrect responses are significantly slower than correct responses: big discrepancies in `log_rt` are obvious in incorrect vs correct plots. \n",
    "\n",
    "This is also supported by the results of our linear regression and ANOVA tests which show a significant F value of 39.316559 for corectness. This finding seems to support part of our hypothesis that corecntess has some positive effect on response times (larger response time = slower). This is a very promising finding from our analysis, and with further study, more robust causal effects of correctness on response times may be found. \n",
    "\n",
    "\n",
    "However, when looking at the the plots presented in Figure 1 again, there do not seem to be similar significant effects of valence and condition on response times. In the plots, there seem to be very minimal (if any) discrepancies in response times when comapring data across valence and condition. This is also supported in the linear regression and ANOVA results showing very small F values for both condition and valence. This finding seems to go against part of our initial hypothesis that valence and condition may have some effect on response times. Though this is not conlusive, it gives some evidence representing the data points in this experiment. Additional analyes will be required to show a more accurate effect of valence and condition on participants' response times. \n",
    "\n",
    "\n",
    "On that note, it is worth noting that though there were no significant findings of valence effects on response times, in the plots in Figure 1, there seemed to be some effect of positive and negative valence vs neutral valence on response times. That is to say that, in the plots, positive and negative valences seemed to have slower response times when compared to neutral valences. This is a very interesting observation, that would require further analysis to verify: perhaps using a linear mixed effects regression will be needed to analyze `abs_valence` to see if this is a valid and significant effect. \n",
    "\n",
    "In sum, we have found reason to believe that corectness has some tangible effect on response times based on the data points we observed. Additionally, valence and condition appeared to have no effects on response times based on our analysis. Though there are some promising plots showing some mild effects of `abs_valence` on response times, it is not conclusive enough to determine a statstically significant effect. More study and analyses will be needed to confirm the accuracy of our findings on both correctness and valence (potentially interacting with condition). \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
