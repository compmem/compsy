--9/10 Lecture

    --General Refresher
        --Can multiply string by number to print that string x number times. 
        --Modulo gives you the remainder. 
        --Use an "if var.isdigit(): " to determine if the input is an integer
        --Remember to +1 to include the final number in the list because it's not inclusive
        --Put the input inside the function
    --Lists
        --Remember that lists use [], and lists times number gives the list that number of times
        --Use index to change object in list. list[0] = 'new input'
        --list[3:] starts at index 3. l[:3] goes until index 3 noninclusive. l[::2] starts at 0 and returns every 2nd value. 
        --list.append('string') adds an individual item
        --list.extend(var) can add the full list stored for var to the end
        --can also just use += ['item']. If you assign as a different variable + is still only concatenation. 
        --pure list assignments a=b change both. Copies made using c=a[:]
        --can determine what some functions do using r.*tab*?
    --Tuples are just immutable lists that use ()
        --min(r) and max(r) return the alphanumerical placement of the letter with " " at 0
        --Starting a function with * returns arguments as a tuple
            --test(*arguments):
                --print(arguments)
            --printall(1, 2.0, 'seven') gives a tuple of those 3. 
        --Can return sequences as a zipped tuple by pairing indices in sequences
            --s = 'xyz'
            --t = [4,5,6]
            --test = zip(s,t)   # This does the pairing, zip will stop at earliest point in either list
            --Would print things like (x,4) or (z,6)
    --Dictionaries (Dict), which relies on keys
        --Best practice is to use tuples, NOT lists because you don't want to switch up the key
        --Values can be mutable tho.
        --Make new dict. dict_name = {
                                  'first': 1,
                                  'second': 2, 
                                  }
        --Change/Add keyval pair: dict_name[key_name] = ''
        --Mutability of dictionaries help easily create histograms (last example on slide)




--9/17 Lecture

    --Overview of what we'll go over
        --What are modules in Python
        --What is 'namespace' and 'variable scope'
        --Import modules and libraries
        --Create N-dimensional arrays
        --Index values in the arrays
        --Perform operations on the arrays
    --Modules
        --Files that have a collection of related fxns (Python has hundreds in Python Standard Library). Can make new ones
            --In psych and neuro, frwquently create new or modify existing modules to fit experiments. 
        --First need to import it into your namespace and then make use of it
            --If want to use trig, (import math) and make sure to convert to Rads. 
            --(math.pi) is a constant value
                --rememebr (math.*tab*) will show you which ones you can use
            --(from module_name import *fxn*) can bring functions in but would override variables that have the same name keeping the one that comes later. Easier to do (math.fxn)
    --Libraries
        --These are larger collections of modules
        --(numpy), (scipy), and (matplotlib)
            --The first two essentially give you matlab
    --(Numpy)
        --Overview
            --Open source extension package for multidimmesional arrays            
            --Gives interface to low level highly optimized libraries for numbers
            --Designed for scientists 
            --Has arrays which contain
                --Discrete time of experiment/situation
                --signal recorded by measuring device
                --pixels
        --Basics
            --Main object is the homogenous array
            --Table of elements {probs numbers} of same time indexed by tuple of numbers
            --Dimensions are "axes" and number of axes are "rank"
            --Coordinates of a point in 3D space [1, 2, 1]:
                --Is an array of rank 1, because it has one axis.
                --That axis has a length of 3.
        --Making arrays
            --Recast list as an array {(a = np.array([0,1,2,3]))}
            --Calling (*array*.len) returns the number of axes {rows}
            --Using (a = np.arange(10)) to make an array from 1-8 {NON-INCLUSIVE}
            --Can also chose lower and upper bounds to divide evenly
                --(c = np.linspace(0,1, 6))
                --(c = np.linspace(0,1, 5, endpoint = False)) gives the same split but doesn't include the last point. 
            --A 3 by 3 matrix with values all == 1
                --(a = np.ones((3, 3)))  # reminder: (3, 3) is a tuple
            --A 2 by 2 matrix with values all == 0
                --(b = np.zeros((2, 2)))
            --A matrix that's 3 by 3 with values == 1 along the diagonal
                --(c = np.eye(3))
            --Mersenne Twisters generate pseudo random numbers
                --(a = np.random.rand(4))
            --Using the code below gives you random yet repeatable numbers
                --(np.random.seed(12345))
                --(print(np.random.rand(3)))
        --Data types
            --Integers run faster than floats, but float is default type
            --Can index in arrays also
            --To replace, use index to alter
                --a[:, 2] = 10 makes all rows at column 2 equal to 10
        --Elementwise operations
            --Just use *numpy_var* *operation* *value*
                --(a = np.array([1, 2, 3, 4]))
                --(a + 1)
            --For Multiplication, CANNOT use c * c to do matrix multiplication
                --Needs to be c @ c
            --Comparative operations can be used to see if each element follows
                --(a == b) sees if each point is equal to the respective in the other
            --Logical operations also run elementwise
                --For "and", "or", and "not" operations, need to use "&", "|", and "~"
        --Linear Algebra
            --Can create triangles in the matrices
            --Can use (a.T) to transpose the matrix by flipping rows and columns
        --Reducing Arrays
            --Can use (np.sum(x)) or x.sum to get sum 




--9/24 Lecture
    --Read & Write basic Text files
        --Reading from files
            --Open files in the same directory using (f = open('file_name.txt', 'r') 
                --{'r' is for read, 'w' is for write, 'a' is for append}
            --To read one line in use (line = f.readline())
            --MAKE SURE TO (f.close()) WHEN DONE
        --Parsing the numbers turns big string into a list of numbers
            --Use (line.strip()) to pull off the new line entry at the end of each line
            --Use (line.split()) to make each val a new entry in a list
                --Can specifiy where to split using (''), (' '), (',') etc.
                --Needs to be run after line.strip
            --To turn a list from string to numbers, can embed for loop into list trrough list comprehension
                --(ints = [int(s) for s in line.strip().split(' ')])
                --List comprehensions are significantly faster than for loops.
            --Can then sort the numbers WITHIN THE SAME LIST using sort
                --(ints.sort())
                --Can sort in descending order using (ints.sort(reverse=True))
        --Writing the file back out
            --Use (f.write()) to file. Can format text with options LOOK INTO THEM
            --Use Random Generators to write using a random fashion
                --(random.shuffle(ints)) is very useful to randomize lists IN PLACE
    --Read and write CSV files {comma separated values}
        --Typical in research which is why it led to module creation; essentially a spreadsheet
        --Classes (csv.DictReader(open('exp_res.csv', 'r'))) helps readily sort created ditionaries
        --May not use this a lot since Pandas is also very useful for what we'll do
    --Pickling objects
        --Dump an object to file for some future use; allows to serialize Python objects
            --Turns them into a byte stream
            --Doesn't produce human readable content, i.e. writes out as binary
            --Use (d2 = pickle.load(open('my_dict.pickle','rb')) to read the binary txt
        --Special things to note
            --Are not portable across languages, usually meant for personal storage
    --Fundamentals of Expt. design
    --Link btwn Science and Coding
        --The human brain is a very complex function; the goal is to change parameters and see what happens 
    --Depend. vs Independ. vars
        --The IV are our inputs, the DV are our outputs. Standard
            --Parameters/input, return/print, assignments. 
    --Constraints on how to structure lists
    --How to make simple list of dicts. to define a trial
    
    
    
    
--10/1
    --ListGen Assignment
        --Goal is to generate the lists that will be presented to subjects to collect responses
        --Hold information that will be presented inside its own dictionary
            --Try to include as much info as possible {Stim, type, cond, novelty}
                --Novelty means is it target or lure
            --For each trial, there is one dictionary. Then all trials are in a mega list
            --The mega list is the key within AN EVEN LARGER dictionary for study or test
            --Study and test dictionaries are held in the ULTIMATE list
            --Lures must be of the same condition as the study material 
        --Each ULTIMATE block should have 2 LARGER dictionaries ()
    --Define Hierarchical State Machine
        --State Machine Interface Library for Experiments AKA SMILE
            --Goal in develpment was: 
                --to have easy and hard takes simple with millisecond accuracy 
                --write expts that run cross-platform 
                --log everything for repetetive cases. 
            --Requires Kivy download {Look at code to install}
                --Cross-platform python application development library
                    --Means you can use Python code-base to deploy app to iOS, Windows, etc.
                --All core librares are compiled to C code, making it very fast
                --Built on OpenGL which results in powerful graphics
        --What is a State Machine in general?
            --Really focused on fintate state machine since not unlimited
            --Common way of modeling systems all over
            --Often represented by a directed graph with nodes as states and edges as transitions
                --Better understood as a stoplight wehre you progress when condition met
            --Powerful extension of a base state machine is to make it hierarchical {HSM}
                --Just means states cane be entire finite state machins
                --HSM can represent almost any computer program
                    --Most computer games are just complex HSMs 
                    --In that state doing something until hit achievement, then go into next connected state
            --Smile helps you build these Sttae machines
    --Difference between build-time and run-time in SMILE
        --The building of the exp and the running of it are two DISTINCT features. 
        --Building/build-times
            --When building states, durations are important
                --If you don't add one, can go on forever unless have conditional break
            --Calls to the SMILE states construct the state machine
            --Acutal values in Python variables will not be available yet
            --Can't evaluate python variables until later so you use references to allow for the evalueation
        --Running/run-times
            --Occurs once you hit (exp.run())
                --Will not run the experiment code again
            --Don't want to run in the file with code to avoid mistakes
                --Use (python exp name.py -s subj001)
                    -- (-s) opens up full screens for tests, (-f) turns off full screen
            --State machine is initialized at the first state and runs to completion
            --Does NOT run code in script, just jumps from state machine to state machine
    --Difference between Action and Flow states in SMILE
        --Action States
            --Carry out some specific input or outpur operation and often have a duration
            --Examples include:
                --(Image): presentes image on screen
                --(Label): Places text on screen
                --(KeyPress): Accepts user input
                --(MovingDot): Presents moving dot stimulus on screen
                --({Shapename}): Can present that shape on the screen 
        --Flow States
            --Control the order of operations for the action states, rare duration
                --Determine how the action state is carried out
                --In a diagram, these are the lines between the bubbles
                --Examples include:
                    --(Parallel) and (Serial): Controll sequences of states
                        --Simultaneous or Sequential respectively
                    --(If), (Elif), (Else): Conditioning
                    --(Meanwhile), (UntilDone): Run states while other states run
                        --To make a block, need to use (with __)
                        --To affect the one above on a done use (with UntilDone():)
    --How to build simple experiments in SMILE
        --For keyboard responses using (kp = KeyPress()), set a (resp_keys)
            --Common ones include "f" or "j"
            --Input as (['F', 'J']), then do KeyPress(keys=resp_keys)
            --Default for escaping an experiment is (SHIFT + Esc)
        --For wait durations can add uniformed randomization
            --Use (jitter) under wait
        --(Loop()) can take in either a number or a list
        --All of the SMILE states secretely log time stamps. 
        
        
        
        
--10/8 
    --General Updates
        --The goal is to be able to step away from the general experimental set ups
            --Create any expt you can think of
         --Recap, SMILE is making a Hierarchical State system that uses flow of data points
            --All expts end up being a directed acyclic graph
            --This is a controlled library so it is often altered/improved/modified
            --One beginning and one end, can trace through the family tree to get A->B
    --How to update SMILE to the latest version
        --Use install line to see if there is a new version of Kivy
            --(conda install -c conda-forge kivy)
        --Update SMILE from github repository
            --(pip install git+https://github.com/compmem/smile --upgrade)
            --Use the front chunk to install but (--upgrade) makes sure using latest version.
        --Understanding Flow
            --Default in SMILE is to proceed sequentially AKA in serial
                --(Serial) Parent state begins when some other thing ends
            --(Parallel) state runs everything at the same time and ends it together
                --Children at starting line and says run. All finished when last one done
                --Can bypass the lagging 'runner' by using (blocking)
                --(UntilDone) and (Meanwhile) are really elaborate blocking styles
                    --(UntilDone) Can establish a condition and change when receive input
                    --(Meanwhile) Establish desire for input and hold condition until met
                        --Seem a little more organized personally
                    --For parents, always using (with)
                        --Still need parents to join
                    --Placement is important, needs to be the parent RIGHT ABOVE
                        --Can be separated by physical lines but any additional not states 
            --Important note!!!
                --When a SMILE crashes, you need to reset the whole kernel
    --How to present images
        --Uses a visual state to present images
            --(Image(source='.jpg'))
        --Default is to present everything at the center of the screen, maintaining pixels
            --When want to change size, use (, allow_stretch=True)
            --To allow for distortion in size change (, keep_ratio=False)
    --To visualize the DAG for an experiment
        --Use dag to give a visual layout of what your code is doing
            --(
                from smile.dag import DAG
                d = DAG(exp)
                d.view_png()
                )
    --How to lay out visual states on the screen
        --Can place them relative to the screen {0,0 in bottom left} OR to other states
        --Space relative to standard black screen is
            --(exp = Experiment(show_splash=False, resolution=(1024,768)))
            --(im_name = Image(source="", left=exp.screen.center_x - var)
        --Can set up visual states to do natural movements for some duration
    --How to include mouse interaction
        --Mouse dissapears by default, can readd using (MouseCursor()) in (Parallel)
        --Track position of mouse for cognitive analysis using:
            --(mrec = Record(mouse_pos=MousePos()))
            -- Remember (|) means "or"
            --Can then create Debug states to make keyword arguments that print at screen
        --For animations, can link the spatial properties of visual spaces with other things
            --Typical is mouse position
    --To log information for easy analysis
        --SMILE states automatically log themselves, meaning you rarely lose info
            --Every single state logs what it does but it's a pain to find and sort
        --More manageable to analyze well-organized log file from your own expt. 
            --Make a list of Key/Value pairs that might be interesting
    --How to write subroutines to organize your code
        --One of most powerful tools for long code is function
            --Mimic the expression of functions using (Subroutine) decorator
            --Decorator is just (@Subroutine) and turns fxn into subroutine
        --Must define at start and have first parameter be 'self'
            --(def MyTrial(self, text):)
    --Elaborating using tests
        --Make sure not to take unputs until the stimuli are up
            --Do this using (Wait(until=stim.appear_time)
            --Use (Log()) to create the trial
        --There are instances where it's ideal to an domize location (like in flanker)
        --Rememebr that Jitter takes a starting value and then goes up to the second val
        --Going to want nested loops but start off with just presenting the stimuli
        --Using Sliding techniques, can pair responses with a block to make it gauge confidence
     --For the DAG portion of the assignment, do conda install pydot




--10/15
    --Reviewing the provided solution to ListGen (Just one possible)
        --He made as general purpose a code as possible
        --Distinctional Effect: If you include an item that stands out more you remember it better.
        --What will you need to transfer?
            --The stuff from the first cell with import csv
            --Cells from "# config variables" down three blocks to "len(blocks)"
    --How to polish up an experiment
        --To start with, ALWAYS make sure your SMILE and Kivy is up to date, upgrade
        --DONT link participant results to their Identification scheme. Keep separate
        --Follow the block under "Subject Info"
            --Relies on an (InputSubject) at the start of the code
        --Can make elaborate instructions to explain expt. but to start, just use label
            --Use (left=exp.screen.left), and can change text alighment with (halign='right')
        --Must make sure that the screen properly accomodates for different window screens. 
            --Uses mechanism called scaling. 
            --Pixel Density is standard are at around 96 pixels per inch. 
            --Import the fxn into the file as shown in Lec. 8
    --How SMILE stores Data
        --Use Slogs which are SMILE (Logs) that have condensed files for a lot of information
            --They are dict lists. Can be read using:
                --(from smile.log import log2dl
                  dl = log2dl('flanker_log.slog'))
    --How to read in slog files
        --Just follow along with the lecture, maybe rewatch. On the mend from test cram
    --Continuing the Flanker Expt. 
        --Use (""" """) to maintain written separations in lines 
        --Don't put the scale in the config, put it in the bottom
        --Once you have the data stored in a list of dictionaries, can use pandas to analyze it
            --(df = pd.DataFrame(dL)) can help it look a lot more manageable. 
                --(df.head()) gives you the first five pieces of info. 
        --Time on screen should be pretty short, around 1.5 sec, use a config variable, no jitter
            --The duration of the intersititial space can have jitter
            --Make them long list bois. 
        --Can add a reminder of the testing configurations between each break




--10/22
    --General Course Plan and Housekeeping
        --From here on out focused on data collection, run the three separate tests.
            --Use computing ID for identifier (normally not do this for anonymity but need grade)
        --Format being used here is identical to that used in Per's actual research lab
        --Updating SMILE software needs upgrade code
            --(conda install -c conda-forge kivy==1.11.1)
            --(pip install git+https://github.com/compmem/smile --upgrade)
            --Update is for Math Distract to help adress using delay period between study and test without having uncontrolled rehearsing techniques confounding variables. 
                --Need to find a way to empty out working memory w stimuli that are orthoganol to the stimuli {If using words, make the delay math using numbers}
    --Read data from slog files
        --Takes a dictionary and pickles it, compresses it, and sends it out to file
            --Can be easily read back in using (log2dl)
        --Pandas is at the core of Data Science work
            --Birthed from "R-code" with a sole purpose of s p e e d
            --Provides 2 main data structures: (Series) and (DataFrames)
            --Key feature of Pandas is that DATA ALIGNMENT IS INTRINSIC
                --The link between labels and data will not be broken unless explicitly stated
    --The Series and DataFrame data structures in Pandas
        --Series are one-dimmensional labeled arrays taht can hold any data type
            --Ints, strings, floats, python objects, etc...
                --Will label with dtype at the end, using values and giving (object) if mixed
            --Axes labels are refered to as the index
                --Non-specific indices will default to 0 based numbers
                --Does not have to be unique {Can repeat indices, but may fail certain tasks}
            --Standard upload practices {like numpy as mp} is (import pandas as pd)
            --Kinda like a np array but also like a dictionary with index/values->key/value pairs. 
                --Can add values from separate indices together as long as they are aligned
                    --If one of the values doesn't exist it will return NaN {AKA DNE}
            --Series essentially just act as one column, but don't often use just one
        --DataFrames are 2-dimensional labeled data structures with multiple columns for different variable recordings {Rows are usually different participants/trials}
            --Many different ways to create a df
                --Can separately make dictionary where keys become columns. Cast as df with indices listed turning into the rows
            --Can use the same base in numpy record arrays, but the manipulation would have been much harder
                --Pick certain columns out of the df treating it like a dict (df['one])
                    --Indices stick when you pull the column which is important
                --Can add columns or manipulate to create new as if dict also
                --To find type make plural (df.dtypes)
                --Remove columns using (del df['name']) or do (df.pop('name'))
            --If a df is given a column with only one thing, will fill the whole column with that one value
    --Load slogs as a DataFrame
        --Can cherry pick columns using a list of separate values (df[['name1', 'name2']])
        --For rows can use something like (df.loc['index_name']) to give series where indices are old columns
            --Can get rows based on numeric placement using (df.iloc[index])
    --Some basic operations on the data 
        --Can get a quick summary of the data being read using (df.describe())
            --Returns columns with useful statistical analyses
            --For reaction times, better to use the log of the rt
                --np.log(df['rt'])
            --As you get bigger, better to reduce column amount by only having important ones
                --(df.columns) to show existing columns
    --(pd.read_excel) will give methods for reading excel sheets
        --Only brings in one sheet at a time tho. 



--29
    --Read in some real data
        --For cell under Diving into Single Subj;
            --In the order data, exp_name, subject, session, slogs
            --Must calculate time between stimulus presentation and reaction time, which requires after-slogging
    --Perform some simple data clean-up
        --Can reverse work to find out which data set comes from which condition
    --Some visualizations with Pandas
    --Simple statistics with SciPy and Stats Models
        --Use SciPy {Scientific Python} to understand the data better
            --NEED TO IMPORT NUMPY JUST TO BE SAFE, FIND OUT HOW TO INSTALL THIS BY GOING BACK AND WATCHING VIDS
        --For legitimacy analysis, ideally use ways of tracking in portion independent of the thing being tested
            --So like for this, you would use the intermediate math blocks. 
            --Especially prevelant for cases like degenerative diseases where chance performance is ACTUAL resp.
        --Use binomial tests to analyze probabilities within two outcomes 
            --Like findind out if a coin flip is biased. 
        --Use t-tests to determine if the data is significantly distributed
            --We only used the non-paired versions 
            --Can't use t-tests on our data as is because the information isn't normally distributed.
                --Becomes more normal if use the numpy log of the values 
        --Other non-parametric statistic methods in SciPi for situations where you don't want to assume a normal distribution. 



--5
    --Behavioral Data Analysis
        --These experiments only have 2 DV, Choice and Reaction Time
        --For these experiments, the thought is that indoor would be remembered better because more distinctive
            --Also an environmental event
            --Distinctive stimuli seem to be most memorable which has evolutionary tones
    --Read in all of the data from experiments
        --Remember to start your files with the imports you'll need
            --(log2dl from smile.log)
            --(from glob import glob) can pull data from initial directory condition ('data/Taskapalooza/s*')
                --The (*) is a wildcard that allows any file added as long as leading end matched. 
                --Does NOT organize the returned lists, just parses quickly 
                --The (?) allows for randomization but only for ONE character. 
                --Good for lots of files that match the naming scheme. 
            --(import os) allows you to manipulate data across systems despite different keystroke schemes
                --(os.path.split)
    --Perform simple data clean-up
        --Lambda is a core variable in python
    --Some more Pandas analysis tricks
        --Can graph problems using a quick plot to analyze corretness or rt values
    --Regresssion across subjects
        --Correlation analysis rely on at least 100 people 
    --Housekeeping
        --Provide methods and results for the experiment you chose
            --Give them all of the information needed to recreate your experiment 
            --Analyze the results and condense. Give a figure and validate with statistical gatherings. 




--12
    --Overview
        --If you find yourself using the same functions over and over, just throw it into a raw (.py) file and then just call it when you need it. It lowers chances of new errors between expts. 
        --Read more into the results you got. For example, if they got it wrong on a lure and chose old, that means they saw a completely new image and had a false alarm that convinced them they had already seen it. 
            --(astype(np.int) converts a boolean into a 0 or 1 int)
    --Basic probability
        --At the core of all stats and quantification of outcome likelihood
            --Integrals of probability function must always be equal to 1
                --If you break the bound, your model is the thing that is incorrect
        --Models can get pretty complicated so we're sticking with continuous models
            --Uniformed distributions establish an equal likelihood of an occurence on either end of a value in a given range
                --This is a really bad model of the world tho. 
                --Law of Large numbers states as the observations go towards infinity, distributino turns Gaussian AKA Normal Dist. 
            --Gausians rely on a standard deviation to scale the function
                --Are better because there is an infintismally small but existant probability going towards infinity 
    --Strength theories of memory
        --It is a very Simple Model (Code for it's not accurate in specific cases, just trends). Assumes baseline familiarity is based on repeated association.
            --In the case of words, the distribution is Gaussian
        --Increasing association strength uniformly shifts the plot to some greater value
    --Relation to Signal Detection Theory
        --Theory developed by radar operators to contrast signal from noise
        --In strength models, you alter criterion based on particular goal {i.e. minimize wrong while maximize right}
    --How to calculate Sensitivity {d' AKA d-prime} and bias {c}
        --Separation of distributions as measured by the distance between Gaussian peaks
            --In units of standard deviation, it's just alpha {shift} divided by deviation 
                --{d'=alpha/sigma}
                --Can use (Z(name_trans)) to get d' but need to adjust because 0 and 1 don't work under Z form. Shifts results slightly but does well enough. 
    --Plotting and statistics w/ d-prime and bias
        --Can install plotline as pn for even more visualization
            --(conda install -c conda-forge plotnine)
        --To plot them with different colors on the pn.ggplot using (fill='name of column')
            --Bar geom needs the 'identity' component
            --Specify next to instead of overlapping by using (position_doge(.9))
        --Split out subplots {like lure vs target within indoor and outdoor performance}
            --Use (pn.facet_wrap('subgroup')
        --For error values, get length by doing mean minus ci and mean plus ci. 
        --Essential theme is to start with a plot and then add on to that plot +=




--19
    --Housekeeping
        --People don't account for uncertainty in models leading to conclusions that don't generalize as they should across a population. 
        --GIVE THE FEEDBACK IN THE CLASS
            --Improve by including literature to help get a grasp of concepts beforehand
        --Final project is a synthesis of the experiment so far. Involves METHODS: a write-up of our task-design and methods and RESULTS: plots with statistics organized based on the question being asked. Short discussion with each plot stating what was found. 
            --The write-up is of ONE of the experiments, and should be detailed enough to allow for replication. 
            --Number of participants was {21 I think} 
            --Make use of the methods being used today!! Can ask a new question but apply the same material. 
    --Quick ANOVA example
        --Let's you rapidly test for key interactions using table information like linear regressions
        --First step in all these ANOVAs is to group results based on subject and condition
            --Goal is to remove valenc though and use actual numeric values. 
        --Important to acknoweldge the possibility of a trial level effect
        --To do it correctly, you need to run one ANOVA run by subject and another run by items. Making sure it is only in one ensures there is no overlap
    --Intro to mixed-effects models
        --Allows you to account for variance at the item and subject levels at the same time
        --Split models into random and fixed effects
            --Random variability may be across subjects or items {like ID of participants or of image showing}
                --If trying to take into account probability of correctness, the measured changes would be fixed
                --If one subject performs really highly and one performs really poorly that would be a random effect
                    --To accurately account for that you would need to allow different subjects to have different intercepts and just measure relative changes. 
                --To fit a model allowing each subject to have an individual slope and intercept would need much more data
                --To get into all of that and include hierarchical models, would be better to take Quant. Cog. 
    --Intro to Bayesian models
        --T-test assess means and standard distributions of different data points to see if there is significant distinction
        --These models generate models using probabiity distributions
            --Expect the two overlapping t graphs to be more conservative for the word count based on the stats shown in the graph
        --Started with a histogram of results, where the area was total probability of results = 1
        --Using rug plots to plot the actual point, can then craft a probability function and extend lines to see where they intercept with the function. {Stem Plot}
    --Fit and visualize a Bayesian mixed-effects regression to our data




































