{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: List Generation for Experiments\n",
    "## Computational Methods in Psychology (and Neuroscience)\n",
    "### Psychology 4500/7559 --- Fall 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "Upon completion of this assignment, the student will have:\n",
    "\n",
    "1. Read in a stimulus pool from a file.\n",
    "\n",
    "2. Randomly generated lists to use in a experiment.\n",
    "\n",
    "3. Written the lists out to files for future use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "* Write code in a Jupyter notebook (after making a copy and renaming it to have your userid in the title --- e.g., A04_ListGen_mst3k).\n",
    "\n",
    "## Design\n",
    "\n",
    "Your assignment is to write a script that reads in a pool of stimuli\n",
    "and creates lists of dictionaries that you will later present to\n",
    "participants as part of an experiment.  \n",
    "\n",
    "The script should be configurable such that you can specify different\n",
    "numbers of lists and trials, along with other details specific to the\n",
    "experiment you decide to do.\n",
    "\n",
    "Each dictionary represents a trial and should contain all the\n",
    "information necessary to identify the stimulus to be presented,\n",
    "details about that stimulus, and the condition in which to present it.\n",
    "This information will be experiment-specific, as outlined below.\n",
    "\n",
    "You have three options for your experiment.  Please select **one** of\n",
    "the following experiments, keeping in mind that your next assignment\n",
    "will be to code the experiment presentation and response collection\n",
    "for the lists you generate from this assignment.\n",
    "\n",
    "  \n",
    "* ***When you are done, save this notebook as HTML (`File -> Download as -> HTML`) and upload it to the matching assignment on UVACollab.***  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Study/Test Block Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from csv import DictReader\n",
    "import copy\n",
    "\n",
    "# function to make a study/test block from the pools past in\n",
    "def gen_block(pools, cond, num_items):\n",
    "    # fill the study list\n",
    "    study_list = []\n",
    "    \n",
    "    # loop over pools\n",
    "    for pool in pools:\n",
    "        # loop over items to add from that pool\n",
    "        # this will be num_items/num_types for mixed lists\n",
    "        for i in range(num_items):\n",
    "            study_item = pool.pop()\n",
    "            study_item.update({'novelty': 'target', \n",
    "                               'cond': cond})\n",
    "            study_list.append(study_item)\n",
    "\n",
    "    # shuffle the study_list\n",
    "    random.shuffle(study_list)\n",
    "    \n",
    "    # copy the study list to be the start of the test list\n",
    "    test_list = copy.deepcopy(study_list)\n",
    "    \n",
    "    # loop over pools\n",
    "    for pool in pools:\n",
    "        # loop over items to add from that pool\n",
    "        # this will be num_items/num_types for mixed lists\n",
    "        for i in range(num_items):\n",
    "            test_item = pool.pop()\n",
    "            test_item.update({'novelty': 'lure', \n",
    "                              'cond': cond})\n",
    "            test_list.append(test_item)\n",
    "    \n",
    "    # shuffle the test list\n",
    "    random.shuffle(test_list)\n",
    "    \n",
    "    return {'study': study_list, 'test': test_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conds: ['neg' 'pos' 'neg' 'neu' 'pos' 'mixed' 'mixed' 'neu' 'pos' 'mixed' 'neu'\n",
      " 'neg']\n",
      "Cond Counts: [3 3 3 3]\n",
      "Num Study: [9 9 9 9 9 9 9 9 9 9 9 9]\n",
      "Num Test: [18 18 18 18 18 18 18 18 18 18 18 18]\n",
      "It passed all the tests!!!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# verification function\n",
    "def verify_blocks(blocks, study_key='study', test_key='test', \n",
    "                  cond_key='cond', mixed_cond='mixed',\n",
    "                  novelty_key='novelty', type_key='in_out'):\n",
    "    # pull out the unique conditions from the first item in each study list\n",
    "    conds = np.array([b[study_key][0][cond_key] for b in blocks])\n",
    "    uconds = np.unique(conds)\n",
    "    num_conds = len(uconds)\n",
    "    print('Conds:', conds)\n",
    "        \n",
    "    # verify number of blocks is multiple of num_conds\n",
    "    assert len(blocks) % num_conds == 0\n",
    "    \n",
    "    # verify each cond is same number of times\n",
    "    cond_counts = np.array([(conds==cond).sum() for cond in uconds])\n",
    "    print('Cond Counts:', cond_counts)\n",
    "    assert np.all((cond_counts - cond_counts[0])==0)\n",
    "\n",
    "    # verify number of study items is always the same\n",
    "    num_study = np.array([len(b[study_key]) for b in blocks])\n",
    "    print('Num Study:', num_study)\n",
    "    assert np.all((num_study - num_study[0])==0)\n",
    "\n",
    "    # verify number of test items is always the same\n",
    "    num_test = np.array([len(b[test_key]) for b in blocks])\n",
    "    print('Num Test:', num_test)\n",
    "    assert np.all((num_test - num_test[0])==0)\n",
    "    \n",
    "    # verify study block is half length of test block\n",
    "    assert np.all((num_study*2 - num_test)==0)\n",
    "    \n",
    "    # do some checks on each block\n",
    "    for b in blocks:\n",
    "        if b[study_key][0][cond_key] == mixed_cond:\n",
    "            # verify mixed lists\n",
    "            # must have same number of each type\n",
    "            types = np.array([item[type_key] for item in b[study_key]])\n",
    "            utypes = np.unique(types)\n",
    "            type_counts = np.array([(types == ut).sum() \n",
    "                                    for ut in utypes])\n",
    "            assert np.all((type_counts - type_counts[0]) == 0)\n",
    "            \n",
    "    print('It passed all the tests!!!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Valence Study\n",
    "\n",
    "The main question of this study is whether recognition memory for\n",
    "words depends on the emotional or affective valence of those words.\n",
    "Participants will study lists of positive (+), negative (-), and\n",
    "neutral (~) words and then, after a short delay, they will be given a\n",
    "recognition test over all the studied target words plus a matched set\n",
    "of non-studied lures.  The stimuli are contained in three separate CSV\n",
    "files:\n",
    "\n",
    "- [Positive Pool](./pos_pool.csv)\n",
    "- [Negative Pool](./neg_pool.csv)\n",
    "- [Neutral Pool](./neu_pool.csv)\n",
    "\n",
    "You will need to read these files in as lists of dictionaries (hint,\n",
    "use the ``DictReader`` from the ``csv`` module that was covered in\n",
    "class.)  Use these pools to create lists of trials for two\n",
    "experimental conditions: pure or mixed.  In the *pure* condition,\n",
    "all of the trials should be words from the same valence (be sure to\n",
    "have the same number of positive, negative, and neutral pure lists.)\n",
    "In the *mixed* condition, each list should contain an equal number of\n",
    "positive, negative, and neutral words in *random* order (hint, use the\n",
    "``shuffle`` function provided by the ``random`` module.) \n",
    "\n",
    "You will need to generate a matching test list for each study list\n",
    "that includes all the studied items, plus a set of lures that match\n",
    "the valence of the studied words.\n",
    "\n",
    "Be sure to add in information to each trial dictionary that identifies\n",
    "the word, its valence, the condition of the list, and whether it is a\n",
    "target or a lure.  Feel free to add in more information if you would\n",
    "like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config variables\n",
    "pos_file = 'pos_pool.csv'\n",
    "neg_file = 'neg_pool.csv'\n",
    "neu_file = 'neu_pool.csv'\n",
    "\n",
    "# number of pools\n",
    "num_pools = 3\n",
    "\n",
    "# number of items in pure lists (must be evenly divisible by num_pools)\n",
    "num_items_pure = 9\n",
    "\n",
    "# number of repetitions of each block type\n",
    "num_reps = 3       \n",
    "\n",
    "# verify these numbers make sense\n",
    "num_items_mixed = int(num_items_pure / num_pools)\n",
    "assert num_items_mixed * num_pools == num_items_pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_pool: 301\n",
      "neg_pool: 292\n",
      "neu_pool: 208\n"
     ]
    }
   ],
   "source": [
    "# load in the pools (must add in valence)\n",
    "pos_pool = [dict({'valence': 'pos'}, **i) \n",
    "            for i in DictReader(open(pos_file, 'r'))]\n",
    "neg_pool = [dict({'valence': 'neg'}, **i) \n",
    "            for i in DictReader(open(neg_file, 'r'))]\n",
    "neu_pool = [dict({'valence': 'neu'}, **i) \n",
    "            for i in DictReader(open(neu_file, 'r'))]\n",
    "\n",
    "# print out number of items in each pool\n",
    "print('pos_pool:', len(pos_pool))\n",
    "print('neg_pool:', len(neg_pool))\n",
    "print('neu_pool:', len(neu_pool))\n",
    "\n",
    "# shuffle the pools\n",
    "random.shuffle(pos_pool)\n",
    "random.shuffle(neg_pool)\n",
    "random.shuffle(neu_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_pool: 229\n",
      "neg_pool: 220\n",
      "neu_pool: 136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the blocks\n",
    "blocks = []\n",
    "for r in range(num_reps):\n",
    "    # generate a pure pos block\n",
    "    blocks.append(gen_block([pos_pool], 'pos', \n",
    "                            num_items_pure))\n",
    "    \n",
    "    # generate a pure neg block\n",
    "    blocks.append(gen_block([neg_pool], 'neg', \n",
    "                            num_items_pure))\n",
    "    \n",
    "    # generate a pure neu block\n",
    "    blocks.append(gen_block([neu_pool], 'neu', \n",
    "                            num_items_pure))\n",
    "    \n",
    "    # generate a mixed pos/neg/neu block\n",
    "    blocks.append(gen_block([pos_pool, neg_pool, neu_pool], \n",
    "                            'mixed', num_items_mixed))\n",
    "\n",
    "# shuffle the blocks\n",
    "random.shuffle(blocks)\n",
    "\n",
    "# let's see how many items we have left\n",
    "print('pos_pool:', len(pos_pool))\n",
    "print('neg_pool:', len(neg_pool))\n",
    "print('neu_pool:', len(neu_pool))\n",
    "\n",
    "len(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'study': [{'valence': 'pos',\n",
       "   'description': 'silly',\n",
       "   'word_no': '981',\n",
       "   'valence_mean': '7.4100000000000001',\n",
       "   'valence_sd': '1.8',\n",
       "   'arousal_mean': '5.8799999999999999',\n",
       "   'arousal_sd': '2.3799999999999999',\n",
       "   'dominance_mean': '6.0',\n",
       "   'dominance_sd': '2.0899999999999999',\n",
       "   'word_frequency': '15',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'ambition',\n",
       "   'word_no': '14',\n",
       "   'valence_mean': '7.04',\n",
       "   'valence_sd': '1.98',\n",
       "   'arousal_mean': '5.6100000000000003',\n",
       "   'arousal_sd': '2.9199999999999999',\n",
       "   'dominance_mean': '6.9299999999999997',\n",
       "   'dominance_sd': '2.0699999999999998',\n",
       "   'word_frequency': '19',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'untroubled',\n",
       "   'word_no': '464',\n",
       "   'valence_mean': '7.6200000000000001',\n",
       "   'valence_sd': '1.4099999999999999',\n",
       "   'arousal_mean': '3.8900000000000001',\n",
       "   'arousal_sd': '2.54',\n",
       "   'dominance_mean': '5.5300000000000002',\n",
       "   'dominance_sd': '2.54',\n",
       "   'word_frequency': '.',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'politeness',\n",
       "   'word_no': '320',\n",
       "   'valence_mean': '7.1799999999999997',\n",
       "   'valence_sd': '1.5',\n",
       "   'arousal_mean': '3.7400000000000002',\n",
       "   'arousal_sd': '2.3700000000000001',\n",
       "   'dominance_mean': '5.7400000000000002',\n",
       "   'dominance_sd': '1.7',\n",
       "   'word_frequency': '5',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'food',\n",
       "   'word_no': '514',\n",
       "   'valence_mean': '7.6500000000000004',\n",
       "   'valence_sd': '1.3700000000000001',\n",
       "   'arousal_mean': '5.9199999999999999',\n",
       "   'arousal_sd': '2.1099999999999999',\n",
       "   'dominance_mean': '6.1799999999999997',\n",
       "   'dominance_sd': '2.48',\n",
       "   'word_frequency': '147',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'wedding',\n",
       "   'word_no': '491',\n",
       "   'valence_mean': '7.8200000000000003',\n",
       "   'valence_sd': '1.5600000000000001',\n",
       "   'arousal_mean': '5.9699999999999998',\n",
       "   'arousal_sd': '2.8500000000000001',\n",
       "   'dominance_mean': '6.6799999999999997',\n",
       "   'dominance_sd': '2.0800000000000001',\n",
       "   'word_frequency': '32',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'inspire',\n",
       "   'word_no': '232',\n",
       "   'valence_mean': '6.9699999999999998',\n",
       "   'valence_sd': '1.9099999999999999',\n",
       "   'arousal_mean': '5.0',\n",
       "   'arousal_sd': '2.5299999999999998',\n",
       "   'dominance_mean': '6.3399999999999999',\n",
       "   'dominance_sd': '2.1099999999999999',\n",
       "   'word_frequency': '3',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'applause',\n",
       "   'word_no': '640',\n",
       "   'valence_mean': '7.5',\n",
       "   'valence_sd': '1.5',\n",
       "   'arousal_mean': '5.7999999999999998',\n",
       "   'arousal_sd': '2.79',\n",
       "   'dominance_mean': '6.4800000000000004',\n",
       "   'dominance_sd': '2.1099999999999999',\n",
       "   'word_frequency': '14',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'warmth',\n",
       "   'word_no': '483',\n",
       "   'valence_mean': '7.4100000000000001',\n",
       "   'valence_sd': '1.8100000000000001',\n",
       "   'arousal_mean': '3.73',\n",
       "   'arousal_sd': '2.3999999999999999',\n",
       "   'dominance_mean': '5.6100000000000003',\n",
       "   'dominance_sd': '1.6699999999999999',\n",
       "   'word_frequency': '28',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'}],\n",
       " 'test': [{'valence': 'pos',\n",
       "   'description': 'circus',\n",
       "   'word_no': '72',\n",
       "   'valence_mean': '7.2999999999999998',\n",
       "   'valence_sd': '1.8400000000000001',\n",
       "   'arousal_mean': '5.9699999999999998',\n",
       "   'arousal_sd': '2.5899999999999999',\n",
       "   'dominance_mean': '5.3899999999999997',\n",
       "   'dominance_sd': '2.25',\n",
       "   'word_frequency': '7',\n",
       "   'novelty': 'lure',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'ambition',\n",
       "   'word_no': '14',\n",
       "   'valence_mean': '7.04',\n",
       "   'valence_sd': '1.98',\n",
       "   'arousal_mean': '5.6100000000000003',\n",
       "   'arousal_sd': '2.9199999999999999',\n",
       "   'dominance_mean': '6.9299999999999997',\n",
       "   'dominance_sd': '2.0699999999999998',\n",
       "   'word_frequency': '19',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'untroubled',\n",
       "   'word_no': '464',\n",
       "   'valence_mean': '7.6200000000000001',\n",
       "   'valence_sd': '1.4099999999999999',\n",
       "   'arousal_mean': '3.8900000000000001',\n",
       "   'arousal_sd': '2.54',\n",
       "   'dominance_mean': '5.5300000000000002',\n",
       "   'dominance_sd': '2.54',\n",
       "   'word_frequency': '.',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'politeness',\n",
       "   'word_no': '320',\n",
       "   'valence_mean': '7.1799999999999997',\n",
       "   'valence_sd': '1.5',\n",
       "   'arousal_mean': '3.7400000000000002',\n",
       "   'arousal_sd': '2.3700000000000001',\n",
       "   'dominance_mean': '5.7400000000000002',\n",
       "   'dominance_sd': '1.7',\n",
       "   'word_frequency': '5',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'applause',\n",
       "   'word_no': '640',\n",
       "   'valence_mean': '7.5',\n",
       "   'valence_sd': '1.5',\n",
       "   'arousal_mean': '5.7999999999999998',\n",
       "   'arousal_sd': '2.79',\n",
       "   'dominance_mean': '6.4800000000000004',\n",
       "   'dominance_sd': '2.1099999999999999',\n",
       "   'word_frequency': '14',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'incentive',\n",
       "   'word_no': '809',\n",
       "   'valence_mean': '7.0',\n",
       "   'valence_sd': '1.72',\n",
       "   'arousal_mean': '5.6900000000000004',\n",
       "   'arousal_sd': '2.4500000000000002',\n",
       "   'dominance_mean': '5.9299999999999997',\n",
       "   'dominance_sd': '2.02',\n",
       "   'word_frequency': '12',\n",
       "   'novelty': 'lure',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'food',\n",
       "   'word_no': '514',\n",
       "   'valence_mean': '7.6500000000000004',\n",
       "   'valence_sd': '1.3700000000000001',\n",
       "   'arousal_mean': '5.9199999999999999',\n",
       "   'arousal_sd': '2.1099999999999999',\n",
       "   'dominance_mean': '6.1799999999999997',\n",
       "   'dominance_sd': '2.48',\n",
       "   'word_frequency': '147',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'glamour',\n",
       "   'word_no': '187',\n",
       "   'valence_mean': '6.7599999999999998',\n",
       "   'valence_sd': '1.6000000000000001',\n",
       "   'arousal_mean': '4.6799999999999997',\n",
       "   'arousal_sd': '2.23',\n",
       "   'dominance_mean': '5.7599999999999998',\n",
       "   'dominance_sd': '2.4900000000000002',\n",
       "   'word_frequency': '5',\n",
       "   'novelty': 'lure',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'grin',\n",
       "   'word_no': '773',\n",
       "   'valence_mean': '7.4000000000000004',\n",
       "   'valence_sd': '1.8700000000000001',\n",
       "   'arousal_mean': '5.2699999999999996',\n",
       "   'arousal_sd': '2.6400000000000001',\n",
       "   'dominance_mean': '6.0',\n",
       "   'dominance_sd': '1.8600000000000001',\n",
       "   'word_frequency': '13',\n",
       "   'novelty': 'lure',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'wedding',\n",
       "   'word_no': '491',\n",
       "   'valence_mean': '7.8200000000000003',\n",
       "   'valence_sd': '1.5600000000000001',\n",
       "   'arousal_mean': '5.9699999999999998',\n",
       "   'arousal_sd': '2.8500000000000001',\n",
       "   'dominance_mean': '6.6799999999999997',\n",
       "   'dominance_sd': '2.0800000000000001',\n",
       "   'word_frequency': '32',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'inspire',\n",
       "   'word_no': '232',\n",
       "   'valence_mean': '6.9699999999999998',\n",
       "   'valence_sd': '1.9099999999999999',\n",
       "   'arousal_mean': '5.0',\n",
       "   'arousal_sd': '2.5299999999999998',\n",
       "   'dominance_mean': '6.3399999999999999',\n",
       "   'dominance_sd': '2.1099999999999999',\n",
       "   'word_frequency': '3',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'friend',\n",
       "   'word_no': '174',\n",
       "   'valence_mean': '7.7400000000000002',\n",
       "   'valence_sd': '1.24',\n",
       "   'arousal_mean': '5.7400000000000002',\n",
       "   'arousal_sd': '2.5699999999999998',\n",
       "   'dominance_mean': '6.7400000000000002',\n",
       "   'dominance_sd': '1.8899999999999999',\n",
       "   'word_frequency': '133',\n",
       "   'novelty': 'lure',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'caress',\n",
       "   'word_no': '64',\n",
       "   'valence_mean': '7.8399999999999999',\n",
       "   'valence_sd': '1.1599999999999999',\n",
       "   'arousal_mean': '5.1399999999999997',\n",
       "   'arousal_sd': '3.0',\n",
       "   'dominance_mean': '5.8300000000000001',\n",
       "   'dominance_sd': '2.1299999999999999',\n",
       "   'word_frequency': '1',\n",
       "   'novelty': 'lure',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'silly',\n",
       "   'word_no': '981',\n",
       "   'valence_mean': '7.4100000000000001',\n",
       "   'valence_sd': '1.8',\n",
       "   'arousal_mean': '5.8799999999999999',\n",
       "   'arousal_sd': '2.3799999999999999',\n",
       "   'dominance_mean': '6.0',\n",
       "   'dominance_sd': '2.0899999999999999',\n",
       "   'word_frequency': '15',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'zest',\n",
       "   'word_no': '1040',\n",
       "   'valence_mean': '6.79',\n",
       "   'valence_sd': '2.04',\n",
       "   'arousal_mean': '5.5899999999999999',\n",
       "   'arousal_sd': '2.6600000000000001',\n",
       "   'dominance_mean': '6.0',\n",
       "   'dominance_sd': '1.99',\n",
       "   'word_frequency': '5',\n",
       "   'novelty': 'lure',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'easygoing',\n",
       "   'word_no': '135',\n",
       "   'valence_mean': '7.2000000000000002',\n",
       "   'valence_sd': '1.5',\n",
       "   'arousal_mean': '4.2999999999999998',\n",
       "   'arousal_sd': '2.52',\n",
       "   'dominance_mean': '5.25',\n",
       "   'dominance_sd': '1.75',\n",
       "   'word_frequency': '1',\n",
       "   'novelty': 'lure',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'useful',\n",
       "   'word_no': '466',\n",
       "   'valence_mean': '7.1399999999999997',\n",
       "   'valence_sd': '1.6000000000000001',\n",
       "   'arousal_mean': '4.2599999999999998',\n",
       "   'arousal_sd': '2.4700000000000002',\n",
       "   'dominance_mean': '5.9299999999999997',\n",
       "   'dominance_sd': '2.1000000000000001',\n",
       "   'word_frequency': '58',\n",
       "   'novelty': 'lure',\n",
       "   'cond': 'pos'},\n",
       "  {'valence': 'pos',\n",
       "   'description': 'warmth',\n",
       "   'word_no': '483',\n",
       "   'valence_mean': '7.4100000000000001',\n",
       "   'valence_sd': '1.8100000000000001',\n",
       "   'arousal_mean': '3.73',\n",
       "   'arousal_sd': '2.3999999999999999',\n",
       "   'dominance_mean': '5.6100000000000003',\n",
       "   'dominance_sd': '1.6699999999999999',\n",
       "   'word_frequency': '28',\n",
       "   'novelty': 'target',\n",
       "   'cond': 'pos'}]}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conds: ['pos' 'mixed' 'mixed' 'neu' 'neu' 'mixed' 'pos' 'pos' 'neg' 'neg' 'neu'\n",
      " 'neg']\n",
      "Cond Counts: [3 3 3 3]\n",
      "Num Study: [9 9 9 9 9 9 9 9 9 9 9 9]\n",
      "Num Test: [18 18 18 18 18 18 18 18 18 18 18 18]\n",
      "It passed all the tests!!!\n"
     ]
    }
   ],
   "source": [
    "verify_blocks(blocks, study_key='study', test_key='test', \n",
    "              cond_key='cond', mixed_cond='mixed',\n",
    "              novelty_key='novelty', type_key='valence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Scene Study\n",
    "\n",
    "This study will test whether recognition memory for indoor and outdoor\n",
    "scenes is modulated by the structure of the study lists.\n",
    "Specifically, participants will study lists that either have indoor\n",
    "and outdoor scenes that come in pure blocks or intermixed (similar to\n",
    "the Valence study above).  The participants will then be given a\n",
    "recognition test over all the studied target images plus a matched set\n",
    "of non-studied lures.  You can access the lists of stimuli available:\n",
    "\n",
    "- [Indoor Pool](./indoor.csv)\n",
    "- [Outdoor Pool](./outdoor.csv)\n",
    "\n",
    "You will need to read these files in as lists of dictionaries (hint,\n",
    "use the ``DictReader`` from the ``csv`` module that was covered in\n",
    "class.)  For the actual experiment we will give you the images that\n",
    "are referenced by the file names in these pools, but for the list\n",
    "generation you do not need the images, themselves and should identify\n",
    "the image you will be presenting using the file name.  Use these pools\n",
    "to create lists of trials for two experimental conditions: pure or\n",
    "mixed.  In the *pure* condition, all of the trials should be images\n",
    "from the same category (be sure to have the same number of indoor\n",
    "and outdoor pure lists.)  In the *mixed* condition, each\n",
    "list should contain an equal number of indoor and outdoor\n",
    "images in *random* order (hint, use the ``shuffle`` function provided\n",
    "by the ``random`` module.)\n",
    "\n",
    "You will need to generate a matching test list for each study list\n",
    "that includes all the studied items, plus a set of lures that match\n",
    "the image categories from the studied items.\n",
    "\n",
    "Be sure to add in information to each trial dictionary that identifies\n",
    "the file name, the category of the image, the condition of the list,\n",
    "and whether it is a target or a lure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config variables\n",
    "indoor_file = 'indoor.csv'\n",
    "outdoor_file = 'outdoor.csv'\n",
    "\n",
    "# number of pools\n",
    "num_pools = 2\n",
    "\n",
    "# number of items in pure lists (must be evenly divisible by num_pools)\n",
    "num_items_pure = 10\n",
    "\n",
    "# number of repetitions of each block type\n",
    "num_reps = 3       \n",
    "\n",
    "# verify these numbers make sense\n",
    "num_items_mixed = int(num_items_pure / num_pools)\n",
    "assert num_items_mixed * num_pools == num_items_pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indoor: 335\n",
      "outdoor: 309\n"
     ]
    }
   ],
   "source": [
    "# load in the pools\n",
    "indoor_pool = [i for i in DictReader(open(indoor_file, 'r'))]\n",
    "outdoor_pool = [i for i in DictReader(open(outdoor_file, 'r'))]\n",
    "print('indoor:', len(indoor_pool))\n",
    "print('outdoor:', len(outdoor_pool))\n",
    "\n",
    "# shuffle the pools\n",
    "random.shuffle(indoor_pool)\n",
    "random.shuffle(outdoor_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indoor: 245\n",
      "outdoor: 219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the blocks\n",
    "blocks = []\n",
    "for r in range(num_reps):\n",
    "    # generate a pure indoor block\n",
    "    blocks.append(gen_block([indoor_pool], 'indoor', \n",
    "                            num_items_pure))\n",
    "    \n",
    "    # generate a pure outdoor block\n",
    "    blocks.append(gen_block([outdoor_pool], 'outdoor', \n",
    "                            num_items_pure))\n",
    "    \n",
    "    # generate a mixed indoor/outdoor block\n",
    "    blocks.append(gen_block([indoor_pool, outdoor_pool], 'mixed', \n",
    "                            num_items_mixed))\n",
    "\n",
    "# shuffle the blocks\n",
    "random.shuffle(blocks)\n",
    "\n",
    "# let's see how many items we have left\n",
    "print('indoor:', len(indoor_pool))\n",
    "print('outdoor:', len(outdoor_pool))\n",
    "\n",
    "len(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'study': [OrderedDict([('filename', 'in0003.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out0008_new.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out0121_new.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out0092_new.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out1355.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'in0035.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'in0083.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'in0236.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out1444.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'in0289.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')])],\n",
       " 'test': [OrderedDict([('filename', 'in0019.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'lure'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out0121_new.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out0008_new.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'in0003.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out1355.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out1444.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out0133_new.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'lure'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'in0321.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'lure'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'in0289.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'in0157.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'lure'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out0092_new.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'in0236.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out0134_new.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'lure'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out1510.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'lure'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'in0373.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'lure'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out1063.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'lure'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'in0035.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'in0083.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'target'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'out1037.jpg'),\n",
       "               ('in_out', 'outdoor'),\n",
       "               ('novelty', 'lure'),\n",
       "               ('cond', 'mixed')]),\n",
       "  OrderedDict([('filename', 'in0250.jpg'),\n",
       "               ('in_out', 'indoor'),\n",
       "               ('novelty', 'lure'),\n",
       "               ('cond', 'mixed')])]}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conds: ['mixed' 'indoor' 'mixed' 'outdoor' 'indoor' 'mixed' 'outdoor' 'outdoor'\n",
      " 'indoor']\n",
      "Cond Counts: [3 3 3]\n",
      "Num Study: [10 10 10 10 10 10 10 10 10]\n",
      "Num Test: [20 20 20 20 20 20 20 20 20]\n",
      "It passed all the tests!!!\n"
     ]
    }
   ],
   "source": [
    "verify_blocks(blocks, study_key='study', test_key='test', \n",
    "              cond_key='cond', mixed_cond='mixed',\n",
    "              novelty_key='novelty', type_key='in_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
